{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67447c14-93a7-4c9a-8027-3ab39704f06e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.73.1-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (72.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.73.1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 2.6/4.3 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 13.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-data-server, grpcio, absl-py, tensorboard\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [grpcio]\n",
      "   -------------------- ------------------- 2/4 [absl-py]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ---------------------------------------- 4/4 [tensorboard]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 grpcio-1.73.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install transformers datasets accelerate bitsandbytes peft trl\n",
    "# !pip install kagglehub\n",
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86a5d3-edfc-41d8-9468-93ba395fda91",
   "metadata": {},
   "source": [
    "## Slect model to fine tune and get tokenizer for that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079595b1-1161-4d6e-9a25-3bb67d12ca07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80a530051604399892656388ff40dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anubh\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfebda6ab2c40d78323ddbeaa43778f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f154342723cd4e6da58e503b4bb709fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using mistral model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e6633-77ad-4782-b6fb-e1e9515d43f8",
   "metadata": {},
   "source": [
    "## Dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68b4543-c73f-48d0-a9fc-dd5261028d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\anubh\\.cache\\kagglehub\\datasets\\viccalexander\\kanyewestverses\\versions\\1\n",
      "Dataset copied to custom location: C:\\Users\\anubh\\Projects\\fine-tune-llm-kanye-best\\my_kanye_data\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# get the model\n",
    "input_file = kagglehub.dataset_download(\"viccalexander/kanyewestverses\")\n",
    "print(\"Path to dataset files:\", input_file)\n",
    "\n",
    "custom_location = os.path.join(os.getcwd(), 'my_kanye_data')\n",
    "os.makedirs(custom_location, exist_ok=True)\n",
    "\n",
    "for item in os.listdir(input_file):\n",
    "    s = os.path.join(input_file, item)\n",
    "    d = os.path.join(custom_location, item)\n",
    "    if os.path.isdir(s):\n",
    "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(s, d)\n",
    "print(f\"Dataset copied to custom location: {custom_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460c3e09-1e87-424c-81ba-eeb0eba0cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to './kanye_bars_prompt_completion.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# split the bars\n",
    "output_filepath = \"./kanye_bars_prompt_completion.jsonl\"\n",
    "input_filepath = f\"{custom_location}/kanye_verses.txt\"\n",
    "\n",
    "with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    current_verse_bars = []\n",
    "    for line_num, line in enumerate(infile):\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        if stripped_line:\n",
    "            current_verse_bars.append(stripped_line)\n",
    "        else:\n",
    "            if current_verse_bars:\n",
    "                for i in range(0, len(current_verse_bars), 2):\n",
    "                    prompt = current_verse_bars[i]\n",
    "                    if i + 1 < len(current_verse_bars):\n",
    "                        completion = current_verse_bars[i+1]\n",
    "                    else:\n",
    "                        completion = prompt\n",
    "\n",
    "                    json_entry = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"completion\": completion\n",
    "                    }\n",
    "                    outfile.write(json.dumps(json_entry, ensure_ascii=False) + '\\n')\n",
    "                current_verse_bars = []\n",
    "\n",
    "    if current_verse_bars:\n",
    "        for i in range(0, len(current_verse_bars), 2):\n",
    "            prompt = current_verse_bars[i]\n",
    "            if i + 1 < len(current_verse_bars):\n",
    "                completion = current_verse_bars[i+1]\n",
    "            else:\n",
    "                completion = prompt\n",
    "\n",
    "            json_entry = {\n",
    "                \"prompt\": prompt,\n",
    "                \"completion\": completion\n",
    "            }\n",
    "            outfile.write(json.dumps(json_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Conversion complete! Output saved to '{output_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa85558-a045-457e-89f9-b7953787e8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ec2b9402214740b3b7bb6f632966a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in original dataset: 3159\n",
      "Samples in training set: 2527\n",
      "Samples in test set: 632\n",
      "\n",
      "Training set examples:\n",
      "{'prompt': 'Or Jay is', 'completion': 'My favorite'}\n",
      "\n",
      "Test set examples:\n",
      "{'prompt': 'Now if my man Benzino got a Benz and they call him Benzino', 'completion': 'When I get my Bentley they gon call me Bent-lino'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "my_dataset = load_dataset('json', data_files=output_filepath)\n",
    "split_dataset = my_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_set = split_dataset['train']\n",
    "test_set = split_dataset['test']\n",
    "\n",
    "print(f\"Total samples in original dataset: {len(my_dataset['train'])}\")\n",
    "print(f\"Samples in training set: {len(train_set)}\")\n",
    "print(f\"Samples in test set: {len(test_set)}\")\n",
    "\n",
    "print(\"\\nTraining set examples:\")\n",
    "print(train_set[0])\n",
    "\n",
    "print(\"\\nTest set examples:\")\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4367da-09e9-4542-b3e0-c9ece5ecc412",
   "metadata": {},
   "source": [
    "## Load model and apply quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec23555-c270-45d5-a641-62e4943dff78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model meta-llama/Llama-3.2-3B with quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb8db59733247539126f120fcec42eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745bee7f02764e53809c0198b0d3e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb756deda15480a88015681819b7bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700ab55228c34e6c92aa768977223046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efddccc6b6b0426ea4a9c81ab6a52ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5478c16f52b486f8ef165ef7ab90cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47208c963f3d463e8a3207cef5285416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model {model_name} with quantization...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={'': 0},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"This likely means the model, even quantized, cannot fit into 6GB VRAM.\")\n",
    "    print(\"Consider a smaller model or running on a cloud GPU.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cdfa3-a26a-48c1-ba56-3e2a202e620c",
   "metadata": {},
   "source": [
    "## Configure training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51bc995b-50ec-4079-86f1-2046fdef7080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a10a57f9e6a40bdb919b72542005f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66233a26e6640d7814785f2aa29128f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:746: UserWarning: Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3dff68bcd9a4537bf2ec953ce8bc237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250c8f20edea4da5bceb5727dc258451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ad5d6a4933482c9643c900dd543a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc52cc6ada3b4cd7ada0b8fc66c6d79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Use SFTConfig to consolidate all training arguments\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"tensorboard\",\n",
    "    eval_steps=100,\n",
    "    max_seq_length=256,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db64585-7bae-4672-ae54-1ff483901b28",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7c705b-0db1-466b-864e-84bf20d0e5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='948' max='948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [948/948 1:16:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.202600</td>\n",
       "      <td>3.373966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.224100</td>\n",
       "      <td>3.377975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.978600</td>\n",
       "      <td>3.403653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=948, training_loss=3.176914617482117, metrics={'train_runtime': 4585.4902, 'train_samples_per_second': 1.653, 'train_steps_per_second': 0.207, 'total_flos': 3049263381196800.0, 'train_loss': 3.176914617482117})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fa645-9888-4078-81e5-0daf2a27f7f7",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32680027-91ac-4c37-adcd-9f87e5df2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama_adapter\\\\tokenizer_config.json',\n",
       " './fine_tuned_llama_adapter\\\\special_tokens_map.json',\n",
       " './fine_tuned_llama_adapter\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the adapter model\n",
    "trainer.model.save_pretrained(\"./fine_tuned_llama_adapter\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a3da4-e424-44c1-bc57-89d4b2bfbc7e",
   "metadata": {},
   "source": [
    "## inference with fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca540416-9825-4190-a0d7-8e8d5fc3a168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: meta-llama/Llama-3.2-3B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f1f4e98fe4367865c73bc5afca252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT adapter and attaching to base model...\n",
      "PEFT adapter loaded and attached.\n",
      "Bougie girl, grab my hand 24/7I know I'm the man\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "OFFLOAD_DIRECTORY = \"./model_offload_cache\"\n",
    "os.makedirs(OFFLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"Loading base model: {model_name}...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer_inference.pad_token is None:\n",
    "    tokenizer_inference.pad_token = tokenizer_inference.eos_token\n",
    "tokenizer_inference.padding_side = \"right\"\n",
    "\n",
    "print(\"Loading PEFT adapter and attaching to base model...\")\n",
    "model_inference = PeftModel.from_pretrained(base_model, \"./fine_tuned_llama_adapter\")\n",
    "print(\"PEFT adapter loaded and attached.\")\n",
    "\n",
    "model_inference.eval()\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_inference,\n",
    "    tokenizer=tokenizer_inference,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "prompt = \"Bougie girl, grab my hand \"\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer_inference.pad_token_id\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331ced0-d435-4cd5-8908-f46fbad39d46",
   "metadata": {},
   "source": [
    "## Push to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193ce75d-c465-480b-ab15-7324e7467061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files from ./fine_tuned_llama_adapter to anubhutiv1/llama_kanye_best...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b144a10ef418bac2e0706779308c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5857b0186cff4a71ad3113eb9d9091f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d48b70477e439298580d0542aadec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter and tokenizer successfully pushed to Hugging Face Hub!\n",
      "You can view your model at: https://huggingface.co/anubhutiv1/llama_kanye_best\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "repo_id = \"anubhutiv1/llama_kanye_best\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "local_adapter_path = \"./fine_tuned_llama_adapter\"\n",
    "\n",
    "print(f\"Uploading files from {local_adapter_path} to {repo_id}...\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=local_adapter_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload initial LoRA adapter and tokenizer\",\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter and tokenizer successfully pushed to Hugging Face Hub!\")\n",
    "print(f\"You can view your model at: https://huggingface.co/{repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
