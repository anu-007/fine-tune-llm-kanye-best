{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67447c14-93a7-4c9a-8027-3ab39704f06e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.73.1-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (72.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\anubh\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.73.1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 2.6/4.3 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 13.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-data-server, grpcio, absl-py, tensorboard\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [grpcio]\n",
      "   -------------------- ------------------- 2/4 [absl-py]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ------------------------------ --------- 3/4 [tensorboard]\n",
      "   ---------------------------------------- 4/4 [tensorboard]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 grpcio-1.73.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install transformers datasets accelerate bitsandbytes peft trl\n",
    "# !pip install kagglehub\n",
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86a5d3-edfc-41d8-9468-93ba395fda91",
   "metadata": {},
   "source": [
    "## Slect model to fine tune and get tokenizer for that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079595b1-1161-4d6e-9a25-3bb67d12ca07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using mistral model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# if there is not pad token in the model add one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e6633-77ad-4782-b6fb-e1e9515d43f8",
   "metadata": {},
   "source": [
    "## Dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e68b4543-c73f-48d0-a9fc-dd5261028d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/viccalexander/kanyewestverses?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106k/106k [00:00<00:00, 171kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: C:\\Users\\anubh\\.cache\\kagglehub\\datasets\\viccalexander\\kanyewestverses\\versions\\1\n",
      "Dataset copied to custom location: C:\\Users\\anubh\\Projects\\fine-tune-llm-kanye-best\\my_kanye_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# get the model\n",
    "input_file = kagglehub.dataset_download(\"viccalexander/kanyewestverses\")\n",
    "print(\"Path to dataset files:\", input_file)\n",
    "\n",
    "custom_location = os.path.join(os.getcwd(), 'my_kanye_data')\n",
    "os.makedirs(custom_location, exist_ok=True)\n",
    "\n",
    "for item in os.listdir(input_file):\n",
    "    s = os.path.join(input_file, item)\n",
    "    d = os.path.join(custom_location, item)\n",
    "    if os.path.isdir(s):\n",
    "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(s, d)\n",
    "print(f\"Dataset copied to custom location: {custom_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460c3e09-1e87-424c-81ba-eeb0eba0cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to './kanye_bars_prompt_completion.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# split the bars\n",
    "output_filepath = \"./kanye_bars_prompt_completion.jsonl\"\n",
    "input_filepath = f\"{custom_location}/kanye_verses.txt\"\n",
    "\n",
    "with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    current_verse_bars = []\n",
    "    for line_num, line in enumerate(infile):\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        if stripped_line:\n",
    "            current_verse_bars.append(stripped_line)\n",
    "        else:\n",
    "            if current_verse_bars:\n",
    "                for i in range(0, len(current_verse_bars), 2):\n",
    "                    prompt = current_verse_bars[i]\n",
    "                    if i + 1 < len(current_verse_bars):\n",
    "                        completion = current_verse_bars[i+1]\n",
    "                    else:\n",
    "                        completion = prompt\n",
    "\n",
    "                    json_entry = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"completion\": completion\n",
    "                    }\n",
    "                    outfile.write(json.dumps(json_entry, ensure_ascii=False) + '\\n')\n",
    "                current_verse_bars = []\n",
    "\n",
    "    if current_verse_bars:\n",
    "        for i in range(0, len(current_verse_bars), 2):\n",
    "            prompt = current_verse_bars[i]\n",
    "            if i + 1 < len(current_verse_bars):\n",
    "                completion = current_verse_bars[i+1]\n",
    "            else:\n",
    "                completion = prompt\n",
    "\n",
    "            json_entry = {\n",
    "                \"prompt\": prompt,\n",
    "                \"completion\": completion\n",
    "            }\n",
    "            outfile.write(json.dumps(json_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Conversion complete! Output saved to '{output_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa85558-a045-457e-89f9-b7953787e8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0ef50bf75f438e9ec3a1a4ea5beee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in original dataset: 3159\n",
      "Samples in training set: 2527\n",
      "Samples in test set: 632\n",
      "\n",
      "Training set examples:\n",
      "{'prompt': 'Or Jay is', 'completion': 'My favorite'}\n",
      "\n",
      "Test set examples:\n",
      "{'prompt': 'Now if my man Benzino got a Benz and they call him Benzino', 'completion': 'When I get my Bentley they gon call me Bent-lino'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "my_dataset = load_dataset('json', data_files=output_filepath)\n",
    "split_dataset = my_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_set = split_dataset['train']\n",
    "test_set = split_dataset['test']\n",
    "\n",
    "print(f\"Total samples in original dataset: {len(my_dataset['train'])}\")\n",
    "print(f\"Samples in training set: {len(train_set)}\")\n",
    "print(f\"Samples in test set: {len(test_set)}\")\n",
    "\n",
    "print(\"\\nTraining set examples:\")\n",
    "print(train_set[0])\n",
    "\n",
    "print(\"\\nTest set examples:\")\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4367da-09e9-4542-b3e0-c9ece5ecc412",
   "metadata": {},
   "source": [
    "## Load model and apply quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec23555-c270-45d5-a641-62e4943dff78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf5a115675d458a922cde9be724866b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decfb36405d8405ca292700efeec4c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f068f816c6a5408a8258191d42487c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f099a9fb82455db479f50ab1c2fe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef52efd4b55484c9fc2dd5e8d975dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c706ea99d04b4efebcc7b322a26bafb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb2ac5b2b334a189cad871a0638653a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (important for QLoRA)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Get the PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters (you'll see a small percentage)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cdfa3-a26a-48c1-ba56-3e2a202e620c",
   "metadata": {},
   "source": [
    "## Configure training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51bc995b-50ec-4079-86f1-2046fdef7080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Directory to save checkpoints and logs\n",
    "    num_train_epochs=3, # Number of training epochs\n",
    "    per_device_train_batch_size=2, # Adjust based on VRAM\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients over multiple steps to simulate larger batch size\n",
    "    gradient_checkpointing=True, # Saves memory\n",
    "    optim=\"paged_adamw_8bit\", # Optimizer optimized for 8-bit training\n",
    "    save_strategy=\"epoch\", # Save checkpoint every epoch\n",
    "    logging_dir=\"./logs\", # Directory for logs\n",
    "    logging_steps=10, # Log every N steps\n",
    "    learning_rate=2e-4, # Fine-tuning learning rate\n",
    "    fp16=True, # Use float16 for mixed precision training\n",
    "    # tf32=False, # Use TF32 for NVIDIA A100+ GPUs\n",
    "    max_grad_norm=0.3, # Clip gradients to prevent exploding gradients\n",
    "    warmup_ratio=0.03, # Linear warmup for learning rate\n",
    "    lr_scheduler_type=\"cosine\", # Learning rate scheduler\n",
    "    disable_tqdm=False, # Enable tqdm progress bars\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    load_best_model_at_end=True, # Load the best model based on evaluation metric\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to monitor for best model\n",
    "    report_to=\"tensorboard\", # Report to TensorBoard\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=256,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db64585-7bae-4672-ae54-1ff483901b28",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d7c705b-0db1-466b-864e-84bf20d0e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='948' max='948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [948/948 1:41:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.854600</td>\n",
       "      <td>2.848207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.482100</td>\n",
       "      <td>2.884391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.758300</td>\n",
       "      <td>3.044556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=948, training_loss=2.43133630229451, metrics={'train_runtime': 6108.0073, 'train_samples_per_second': 1.241, 'train_steps_per_second': 0.155, 'total_flos': 9247608244346880.0, 'train_loss': 2.43133630229451})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fa645-9888-4078-81e5-0daf2a27f7f7",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32680027-91ac-4c37-adcd-9f87e5df2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_mistral_adapter\\\\tokenizer_config.json',\n",
       " './fine_tuned_mistral_adapter\\\\special_tokens_map.json',\n",
       " './fine_tuned_mistral_adapter\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the adapter model\n",
    "trainer.model.save_pretrained(\"./fine_tuned_mistral_adapter\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_mistral_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a3da4-e424-44c1-bc57-89d4b2bfbc7e",
   "metadata": {},
   "source": [
    "## inference with fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca540416-9825-4190-a0d7-8e8d5fc3a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b135fe6e03143818d0ac04d771575a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.3.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.3.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.3.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.3.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.4.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.4.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.4.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.4.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.5.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.5.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.5.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.5.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.6.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.6.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.6.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.6.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.7.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.7.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.7.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.7.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.8.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.8.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.8.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.8.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.9.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.9.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.9.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.9.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.10.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.10.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.10.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.10.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\anubh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.norm, model.rotary_emb, lm_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OFFLOAD_DIRECTORY, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load base model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_inference \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_mistral_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m     16\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     offload_folder\u001b[38;5;241m=\u001b[39mOFFLOAD_DIRECTORY,\n\u001b[0;32m     18\u001b[0m     offload_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m tokenizer_inference \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_mistral_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_inference\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4947\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4946\u001b[0m     adapter_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m key_mapping\n\u001b[1;32m-> 4947\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_adapter(\n\u001b[0;32m   4948\u001b[0m         _adapter_model_path,\n\u001b[0;32m   4949\u001b[0m         adapter_name\u001b[38;5;241m=\u001b[39madapter_name,\n\u001b[0;32m   4950\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   4951\u001b[0m         adapter_kwargs\u001b[38;5;241m=\u001b[39madapter_kwargs,\n\u001b[0;32m   4952\u001b[0m     )\n\u001b[0;32m   4954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[0;32m   4955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\peft.py:307\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[1;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    306\u001b[0m ):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_accelerate_model(\n\u001b[0;32m    308\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m    309\u001b[0m         max_memory\u001b[38;5;241m=\u001b[39mmax_memory,\n\u001b[0;32m    310\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m    311\u001b[0m         offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[0;32m    312\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\peft.py:558\u001b[0m, in \u001b[0;36mPeftAdapterMixin._dispatch_accelerate_model\u001b[1;34m(self, device_map, max_memory, offload_folder, offload_index)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    555\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28mself\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39mmax_memory, no_split_module_classes\u001b[38;5;241m=\u001b[39mno_split_module_classes\n\u001b[0;32m    557\u001b[0m     )\n\u001b[1;32m--> 558\u001b[0m dispatch_model(\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    560\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m    561\u001b[0m     offload_dir\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdispatch_model_kwargs,\n\u001b[0;32m    563\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\big_modeling.py:383\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    381\u001b[0m disk_modules \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, device \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(disk_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to be offloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(disk_modules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mlen\u001b[39m(disk_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(offload_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(offload_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m    391\u001b[0m ):\n\u001b[0;32m    392\u001b[0m     disk_state_dict \u001b[38;5;241m=\u001b[39m extract_submodules_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict(), disk_modules)\n",
      "\u001b[1;31mValueError\u001b[0m: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.norm, model.rotary_emb, lm_head."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "OFFLOAD_DIRECTORY = \"./model_offload_cache\"\n",
    "os.makedirs(OFFLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Load base model\n",
    "model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./fine_tuned_mistral_adapter\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=OFFLOAD_DIRECTORY,\n",
    "    offload_state_dict=True\n",
    ")\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(\"./fine_tuned_mistral_adapter\")\n",
    "if tokenizer_inference.pad_token is None:\n",
    "    tokenizer_inference.pad_token = tokenizer_inference.eos_token\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_inference,\n",
    "    tokenizer=tokenizer_inference,\n",
    "    torch_dtype=torch.float16, # Or bfloat16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "prompt = \"### Prompt: i just woke up in the morning \\n ### completion:\"\n",
    "# Ensure the prompt format matches your training data!\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer_inference.pad_token_id\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe05df2-bfba-4d0e-afb5-29d57596350e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     17\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     18\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, \u001b[38;5;66;03m# Or torch.float16 if that's what you used\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 1. Load the original BASE model (potentially with quantization)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m base_model_inference \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     26\u001b[0m     base_model_name_for_inference, \u001b[38;5;66;03m# Load from Hugging Face Hub\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config, \u001b[38;5;66;03m# Include if you used QLoRA during training\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, \u001b[38;5;66;03m# Match the dtype you want for operations\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     offload_folder\u001b[38;5;241m=\u001b[39mOFFLOAD_DIRECTORY, \u001b[38;5;66;03m# Offload base model if it doesn't fit\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     offload_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m tokenizer_inference \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_name_for_inference)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_inference\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4820\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4820\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n\u001b[0;32m   4822\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[0;32m   4823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1460\u001b[0m, in \u001b[0;36m_get_device_map\u001b[1;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[0;32m   1457\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1460\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:117\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel # Import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "OFFLOAD_DIRECTORY = \"./model_offload_cache\"\n",
    "os.makedirs(OFFLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# You NEED to specify the original base model name here\n",
    "# This should be the same as 'model_name' you used during training\n",
    "base_model_name_for_inference = \"mistralai/Mistral-7B-v0.1\" # <--- IMPORTANT: Replace with your actual base model name\n",
    "\n",
    "# If you trained with 4-bit/8-bit quantization (QLoRA),\n",
    "# you should load the base model with the same quantization config.\n",
    "# If you didn't use quantization for the base model during training, remove bnb_config.\n",
    "# Assuming you did use QLoRA, include this:\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Or torch.float16 if that's what you used\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 1. Load the original BASE model (potentially with quantization)\n",
    "base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name_for_inference, # Load from Hugging Face Hub\n",
    "    quantization_config=bnb_config, # Include if you used QLoRA during training\n",
    "    torch_dtype=torch.float16, # Match the dtype you want for operations\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=OFFLOAD_DIRECTORY, # Offload base model if it doesn't fit\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(base_model_name_for_inference)\n",
    "if tokenizer_inference.pad_token is None:\n",
    "    tokenizer_inference.pad_token = tokenizer_inference.eos_token\n",
    "\n",
    "# 2. Load the PEFT adapter weights on top of the base model\n",
    "model_inference = PeftModel.from_pretrained(base_model_inference, \"./fine_tuned_mistral_adapter\")\n",
    "\n",
    "# You can optionally call .eval() for inference\n",
    "model_inference = model_inference.eval()\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_inference,\n",
    "    tokenizer=tokenizer_inference,\n",
    "    torch_dtype=torch.float16, # Or bfloat16\n",
    "    device_map=\"auto\" # This applies to the combined model\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "prompt = \"### Prompt: i just woke up in the morning \\n ### completion:\"\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer_inference.pad_token_id\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
